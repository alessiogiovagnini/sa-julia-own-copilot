{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved as train_set.csv, test_set.csv, and eval_set.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data into a Pandas DataFrame\n",
    "file_path = \"merged_julia_final.csv\"  # Update this path if necessary\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Split the data into training, test, and evaluation sets (e.g., 70%, 20%, 10%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "test_df, eval_df = train_test_split(temp_df, test_size=0.333, random_state=42)  # 0.333 * 30% = ~10%\n",
    "\n",
    "# Save each subset to a separate CSV file\n",
    "train_df.to_csv(\"train_set.csv\", index=False)\n",
    "test_df.to_csv(\"test_set.csv\", index=False)\n",
    "eval_df.to_csv(\"eval_set.csv\", index=False)\n",
    "\n",
    "print(\"Datasets saved as train_set.csv, test_set.csv, and eval_set.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved as train_set_doc.csv, test_set_doc.csv, and eval_set_doc.csv.\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "# Split the data into training, test, and evaluation sets (e.g., 70%, 20%, 10%)\n",
    "train_df_doc, temp_df_doc = train_test_split(df, test_size=0.3, random_state=42)\n",
    "test_df_doc, eval_df_doc = train_test_split(temp_df_nodoc, test_size=0.333, random_state=42)  # 0.333 * 30% = ~10%\n",
    "\n",
    "# Save each subset to a separate CSV file\n",
    "train_df_doc.to_csv(\"train_set_doc.csv\", index=False)\n",
    "test_df_doc.to_csv(\"test_set_doc.csv\", index=False)\n",
    "eval_df_doc.to_csv(\"eval_set_doc.csv\", index=False)\n",
    "\n",
    "print(\"Datasets saved as train_set_doc.csv, test_set_doc.csv, and eval_set_doc.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 697571\n",
      "Number of rows in test set: 199406\n",
      "Number of rows in evaluation set: 99554\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")\n",
    "print(f\"Number of rows in evaluation set: {len(eval_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 125048\n",
      "Number of rows in test set: 35745\n",
      "Number of rows in evaluation set: 17847\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows in training set: {len(train_df_doc)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df_doc)}\")\n",
    "print(f\"Number of rows in evaluation set: {len(eval_df_doc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine docstring and function signature as input\n",
    "train_df['input_text'] = train_df['function_signature']\n",
    "train_df['target_text'] = train_df['function_body']\n",
    "        \n",
    "eval_df['input_text'] = eval_df['function_signature']\n",
    "eval_df['target_text'] = eval_df['function_body']\n",
    "        \n",
    "test_df['input_text'] = test_df['function_signature']\n",
    "test_df['target_text'] = test_df['function_body']\n",
    "        \n",
    "# # Split into training, evaluation, and test sets\n",
    "trainingSetInputs = train_df['input_text'].tolist()\n",
    "trainingSetTargets = train_df['target_text'].tolist()\n",
    "        \n",
    "evaluationSetInputs = eval_df['input_text'].tolist()\n",
    "evalutionSetTargets = eval_df['target_text'].tolist()\n",
    "        \n",
    "testSetInputs = test_df['input_text'].tolist()\n",
    "testSetTargets = test_df['target_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine docstring and function signature as input\n",
    "train_df_doc['input_text'] = train_df_doc['docstring'] + \"\\n\" + train_df_doc['function_signature']\n",
    "train_df_doc['target_text'] = train_df_doc['function_body']\n",
    "        \n",
    "eval_df_doc['input_text'] = eval_df_doc['docstring'] + \"\\n\" + eval_df_doc['function_signature']\n",
    "eval_df_doc['target_text'] = eval_df_doc['function_body']\n",
    "        \n",
    "test_df_doc['input_text'] = test_df_doc['docstring'] + \"\\n\" + test_df_doc['function_signature']\n",
    "test_df_doc['target_text'] = test_df_doc['function_body']\n",
    "        \n",
    "# # Split into training, evaluation, and test sets\n",
    "trainingSetDocInputs = train_df_doc['input_text'].tolist()\n",
    "trainingSetDocTargets = train_df_doc['target_text'].tolist()\n",
    "        \n",
    "evaluationSetDocInputs = eval_df_doc['input_text'].tolist()\n",
    "evalutionSetDocTargets = eval_df_doc['target_text'].tolist()\n",
    "        \n",
    "testSetDocInputs = test_df_doc['input_text'].tolist()\n",
    "testSetDocTargets = test_df_doc['target_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataset(input_text, target_text, tokenizer, tokens_per_batch=512):\n",
    "    # Set padding token if not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    max_length = tokens_per_batch  # Define a reasonable max_length for inputs and targets\n",
    "\n",
    "    # Tokenize the input and target texts\n",
    "    input_encodings = tokenizer(input_text, padding='max_length', truncation=True, return_tensors='pt', max_length=max_length).to(\"cuda\")\n",
    "    target_encodings = tokenizer(target_text, padding='max_length', truncation=True, return_tensors='pt', max_length=max_length).to(\"cuda\")\n",
    "    \n",
    "    # Ensure labels are aligned with input\n",
    "    target_ids = target_encodings['input_ids']\n",
    "    target_ids[target_ids == tokenizer.pad_token_id] = -100  # Ignore padding tokens in the loss computation\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_ids\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without docstring\n",
    "trainingSet = makeDataset(trainingSetInputs, trainingSetTargets, tokenizer)\n",
    "evaluationSet = makeDataset(evaluationSetInputs, evalutionSetTargets, tokenizer)\n",
    "testSet = makeDataset(testSetInputs, testSetTargets, tokenizer)\n",
    "\n",
    "#with docstring\n",
    "trainingSetDoc = makeDataset(trainingSetDocInputs, trainingSetDocTargets, tokenizer)\n",
    "evaluationSetDoc = makeDataset(evaluationSetDocInputs, evalutionSetDocTargets, tokenizer)\n",
    "testSetDoc = makeDataset(testSetDocInputs, testSetDocTargets, tokenizer)\n",
    "\n",
    "\n",
    "trainingSet.save_to_disk(\"datasets\")\n",
    "evaluationSet.save_to_disk(\"datasets\")\n",
    "testSet.save_to_disk(\"datasets\")\n",
    "\n",
    "trainingSetDoc.save_to_disk(\"datasets\")\n",
    "evaluationSetDoc.save_to_disk(\"datasets\")\n",
    "testSetDoc.save_to_disk(\"datasets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAA",
   "language": "python",
   "name": "vaa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
