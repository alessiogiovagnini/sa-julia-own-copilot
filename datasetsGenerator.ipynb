{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SA24-G2/sa-julia-own-copilot/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a Pandas DataFrame\n",
    "file_path = \"filtered_final.csv\"  # Update this path if necessary\n",
    "train_df_doc = pd.read_csv(file_path) #read the data\n",
    "train_df_doc = train_df_doc.dropna() #drop null values in columns\n",
    "train_df_doc = train_df_doc.sample(frac=1).reset_index(drop=True) #shuffle the dataset\n",
    "train_df_doc = train_df_doc.head(100000) #limit the dataset to 100'000\n",
    "# train_df_doc = train_df_doc.head(100) #limit the dataset to 100'000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set with docstring: 100000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows in training set with docstring: {len(train_df_doc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine docstring and function signature as input\n",
    "train_df_doc['input_text_doc'] = '<|im_start|>' + '\"\"\"' + train_df_doc['docstring'] + '\\n\"\"\"' + '\\n' + train_df_doc['function_signature'] + '\\n' + train_df_doc['function_body'] + '<|im_end|>'\n",
    "train_df_doc['input_text'] = '<|im_start|>' + train_df_doc['function_signature'] + '\\n' + train_df_doc['function_body'] + '<|im_end|>'\n",
    "\n",
    "# # Split into training, evaluation, and test sets\n",
    "trainingSetDocInputs = train_df_doc['input_text'].tolist()\n",
    "trainingSetInputs = train_df_doc['input_text_doc'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataset(input_text, tokenizer):\n",
    "    # Set padding token if not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    max_length = 1024  # Define a reasonable max_length for inputs and targets\n",
    "\n",
    "    # Tokenize the input and target texts\n",
    "    input_encodings = tokenizer(input_text, padding='max_length', truncation=True, return_tensors='pt', max_length=max_length)\n",
    "    input_encodings['labels'] = input_encodings['input_ids'].clone()\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': input_encodings['labels']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 135M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████| 100000/100000 [00:01<00:00, 93305.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#without docstring\n",
    "model_name = 'HuggingFaceTB/SmolLM-135M'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "trainingSet = makeDataset(trainingSetInputs, tokenizer)\n",
    "trainingSet.save_to_disk(\"./datasets/135MTrainSet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████| 100000/100000 [00:00<00:00, 119261.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#with docstring\n",
    "model_name = 'HuggingFaceTB/SmolLM-135M'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "trainingSetDoc = makeDataset(trainingSetDocInputs, tokenizer)\n",
    "trainingSetDoc.save_to_disk(\"./datasets/135MDocTrainSet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 360M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████| 100000/100000 [00:01<00:00, 92081.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#without docstring\n",
    "model_name = 'HuggingFaceTB/SmolLM-360M'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "trainingSet = makeDataset(trainingSetInputs, tokenizer)\n",
    "trainingSet.save_to_disk(\"./datasets/360MTrainSet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████| 100000/100000 [00:00<00:00, 122069.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#with docstring\n",
    "model_name = 'HuggingFaceTB/SmolLM-360M'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "trainingSetDoc = makeDataset(trainingSetDocInputs, tokenizer)\n",
    "trainingSetDoc.save_to_disk(\"./datasets/360MDocTrainSet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████| 100000/100000 [00:01<00:00, 59057.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#without docstring\n",
    "model_name = 'HuggingFaceTB/SmolLM-1.7B'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "trainingSet = makeDataset(trainingSetInputs, tokenizer)\n",
    "trainingSet.save_to_disk(\"./datasets/1.7BTrainSet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████| 100000/100000 [00:01<00:00, 79562.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#with docstring\n",
    "model_name = 'HuggingFaceTB/SmolLM-1.7B'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "trainingSetDoc = makeDataset(trainingSetDocInputs, tokenizer)\n",
    "trainingSetDoc.save_to_disk(\"./datasets/1.7BDocTrainSet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
